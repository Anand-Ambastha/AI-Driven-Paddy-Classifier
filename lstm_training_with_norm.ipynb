{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL_WQKYfvWoc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import pandas as pd\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content\n"
      ],
      "metadata": {
        "id": "VRzGRc4mL4m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw_VXdXIz-8t"
      },
      "outputs": [],
      "source": [
        "!ls -lh /content/X.npy\n",
        "!ls -lh /content/X_test.npy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5VsuwztHp-f"
      },
      "outputs": [],
      "source": [
        "X = np.load(\"/content/X.npy\")\n",
        "print(\"Shape:\", X.shape)\n",
        "print(\"Dtype:\", X.dtype)\n",
        "\n",
        "x_test = np.load(\"/content/X_test.npy\")\n",
        "print(\"Shape:\", x_test.shape)\n",
        "print(\"Dtype:\", x_test.dtype)\n",
        "\n",
        "\n",
        "y = np.load(\"/content/y.npy\")\n",
        "print(\"Shape:\", y.shape)\n",
        "print(\"Dtype:\", y.dtype)\n",
        "\n",
        "\n",
        "y_test = np.load(\"/content/y_test.npy\")\n",
        "print(\"Shape:\", y_test.shape)\n",
        "print(\"Dtype:\", y_test.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jagadish Scoring (48h)**\n",
        "\n",
        "\n",
        "\n",
        "1.   +2 if max temp > 35°C\n",
        "2.   +1 if max temp > 33°C\n",
        "3.   +1 if rainfall < 2 mm\n",
        "4.   +1 if ≥2 consecutive hot days (>33°C)\n",
        "5.   +1 if mean VPD > 2.5 kPa\n",
        "\n",
        "**Hybrid Label**\n",
        "\n",
        "1. High (2): score ≥ 4 or deficit ≥ 40\n",
        "2. Medium (1): score ≥ 2 or deficit ≥ 15\n",
        "3. Low (0): otherwise"
      ],
      "metadata": {
        "id": "9MZ2zyz5jqYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmGdCDfDvf4f"
      },
      "outputs": [],
      "source": [
        "site_ids = np.load(\"/content/site_ids.npy\")\n",
        "site_ids_test = np.load(\"/content/site_ids_test.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRxpnh2-vqIR"
      },
      "outputs": [],
      "source": [
        "print(\"Data shapes:\", site_ids.shape, site_ids_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grNRCCqVvqlg"
      },
      "outputs": [],
      "source": [
        "n_features = X.shape[2]\n",
        "n_classes = len(np.unique(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahm6R3sjvvWo"
      },
      "outputs": [],
      "source": [
        "n_total = len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN4K3SIVJCmZ"
      },
      "outputs": [],
      "source": [
        "gss = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(gss.split(X, y, groups=site_ids))\n",
        "\n",
        "X_train = X[train_idx]\n",
        "y_train = y[train_idx]\n",
        "site_ids_train = site_ids[train_idx]\n",
        "\n",
        "X_val   = X[val_idx]\n",
        "y_val   = y[val_idx]\n",
        "site_ids_val = site_ids[val_idx]\n",
        "\n",
        "\n",
        "print(\"Train:\", X_train.shape, y_train.shape , site_ids_train.shape)\n",
        "print(\"Val:  \", X_val.shape, y_val.shape , site_ids_val.shape)\n",
        "print(\"Test: \", x_test.shape, y_test.shape, site_ids_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bZUHQs9v1Dh"
      },
      "outputs": [],
      "source": [
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8-OeRkQIYtr"
      },
      "outputs": [],
      "source": [
        "def normalize_per_location(X, site_ids):\n",
        "    \"\"\"\n",
        "    Normalize each feature per location using mean and std computed over\n",
        "    all samples and timesteps from that location.\n",
        "\n",
        "    Parameters\n",
        "    X : np.ndarray\n",
        "        Shape (num_samples, timesteps, num_features)\n",
        "    site_ids : np.ndarray\n",
        "        Shape (num_samples,). Each entry is a string or int identifying the site.\n",
        "\n",
        "    Returns\n",
        "    X_norm : np.ndarray\n",
        "        Normalized array of the same shape as X.\n",
        "    stats : dict\n",
        "        Dictionary of {location: (mean, std)} for reproducibility or test normalization.\n",
        "    \"\"\"\n",
        "    X_norm = np.zeros_like(X)\n",
        "    stats = {}\n",
        "\n",
        "    for loc in np.unique(site_ids):\n",
        "        idx = np.where(site_ids == loc)[0]\n",
        "        loc_data = X[idx]\n",
        "\n",
        "        mean = loc_data.mean(axis=(0, 1), keepdims=True)\n",
        "        std = loc_data.std(axis=(0, 1), keepdims=True) + 1e-6\n",
        "\n",
        "        X_norm[idx] = (loc_data - mean) / std\n",
        "        stats[loc] = (mean, std)\n",
        "\n",
        "    return X_norm, stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLwUqlCKLOwT"
      },
      "outputs": [],
      "source": [
        "def test_normalize_per_location(X_test,site_ids_test):\n",
        "    X_test_norm = np.zeros_like(X_test)\n",
        "    for loc in np.unique(site_ids_test):\n",
        "        idx = np.where(site_ids_test == loc)[0]\n",
        "        loc_data = X_test[idx]\n",
        "\n",
        "        if loc in stats:\n",
        "            mean, std = stats[loc]\n",
        "        else:\n",
        "            mean = loc_data.mean(axis=(0, 1), keepdims=True)\n",
        "            std  = loc_data.std(axis=(0, 1), keepdims=True) + 1e-6\n",
        "\n",
        "        X_test_norm[idx] = (loc_data - mean) / std\n",
        "\n",
        "    return X_test_norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM0N6VXqIua9"
      },
      "outputs": [],
      "source": [
        "X_train, stats = normalize_per_location(X_train, site_ids_train)\n",
        "X_val, _ = normalize_per_location(X_val, site_ids_val)\n",
        "x_test = test_normalize_per_location(x_test, site_ids_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline**"
      ],
      "metadata": {
        "id": "Gk8ADijdY6p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import joblib\n",
        "\n",
        "\n",
        "n_train = X_train.shape[0]\n",
        "n_test  = x_test.shape[0]\n",
        "Xtr_flat = X_train.reshape(n_train, -1)\n",
        "Xte_flat = x_test.reshape(n_test, -1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xtr_flat_s = scaler.fit_transform(Xtr_flat)\n",
        "Xte_flat_s = scaler.transform(Xte_flat)\n",
        "\n",
        "\n",
        "print(\"\\n=== Logistic Regression ===\")\n",
        "lr = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"saga\", C=1.0, n_jobs=-1)\n",
        "lr.fit(Xtr_flat_s, y_train)\n",
        "y_pred = lr.predict(Xte_flat_s)\n",
        "print(\"LR - Acc:\", accuracy_score(y_test, y_pred), \"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(lr, \"baseline_lr.joblib\")\n",
        "\n",
        "print(\"\\n=== Random Forest ===\")\n",
        "rf = RandomForestClassifier(n_estimators=300, max_depth=15, n_jobs=-1, random_state=42)\n",
        "rf.fit(Xtr_flat, y_train)\n",
        "y_pred = rf.predict(Xte_flat)\n",
        "print(\"RF - Acc:\", accuracy_score(y_test, y_pred), \"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(classification_report(y_test, y_pred))\n",
        "joblib.dump(rf, \"baseline_rf.joblib\")\n",
        "\n"
      ],
      "metadata": {
        "id": "epmhrYsYS5dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== 1-layer LSTM ===\")\n",
        "num_classes = len(np.unique(y))\n",
        "ytr_cat = to_categorical(y_train, num_classes)\n",
        "yte_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model1.add(LSTM(64, return_sequences=False))\n",
        "model1.add(Dense(num_classes, activation=\"softmax\"))\n",
        "model1.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "history = model1.fit(X_train, ytr_cat, validation_split=0.1, epochs=50, batch_size=64, callbacks=[es], verbose=2)\n",
        "y_pred_proba = model1.predict(x_test)\n",
        "y_pred_b = np.argmax(y_pred_proba, axis=1)\n",
        "print(\"LSTM - Acc:\", accuracy_score(y_test, y_pred_b), \"Macro-F1:\", f1_score(y_test, y_pred_b, average=\"macro\"))\n",
        "print(classification_report(y_test, y_pred_b))\n",
        "model.save(\"baseline_lstm.h5\")\n",
        "\n",
        "joblib.dump(scaler, \"scaler.joblib\")"
      ],
      "metadata": {
        "id": "L9ie08CMfRT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Model**"
      ],
      "metadata": {
        "id": "hD_YNmg6ZjZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP_YizElv12c"
      },
      "outputs": [],
      "source": [
        "class SumPooling1D(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_sum(inputs, axis=1)\n",
        "\n",
        "def attention_block(inputs):\n",
        "    score = layers.Dense(1, activation=\"tanh\", name=\"att_score\")(inputs)\n",
        "    weights = layers.Softmax(axis=1, name=\"att_weights\")(score)  # ← NAME FIXED\n",
        "    context = layers.Multiply(name=\"att_apply\")([inputs, weights])\n",
        "    context = SumPooling1D(name=\"att_context\")(context)\n",
        "\n",
        "\n",
        "    return context, weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-pHAq-mv58w"
      },
      "outputs": [],
      "source": [
        "def build_bilstm_attention_model(n_features, n_classes, timesteps=48):\n",
        "    inputs = layers.Input(shape=(timesteps, n_features))\n",
        "    x = layers.BatchNormalization()(inputs)\n",
        "\n",
        "    x = layers.Bidirectional(\n",
        "        layers.LSTM(64, return_sequences=True,\n",
        "                    dropout=0.2, recurrent_dropout=0.2)\n",
        "    )(x)\n",
        "\n",
        "    x = layers.LSTM(32, return_sequences=True,\n",
        "                    dropout=0.2, recurrent_dropout=0.2)(x)\n",
        "\n",
        "    # attention\n",
        "    context, attn_weights = attention_block(x)\n",
        "\n",
        "    # classification head\n",
        "    x = layers.Dense(32, activation=\"relu\",\n",
        "                     kernel_regularizer=regularizers.l2(1e-4))(context)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(n_classes, activation=\"softmax\", name=\"pred\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs=[outputs, attn_weights], name=\"BiLSTM_Attention\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={\"pred\": \"sparse_categorical_crossentropy\", \"att_weights\": None},\n",
        "        loss_weights={\"pred\": 1.0, \"att_weights\": 0.0},\n",
        "        metrics={\"pred\": \"accuracy\"}\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CucS-RQ8VeFp"
      },
      "outputs": [],
      "source": [
        "model = build_bilstm_attention_model(n_features, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5dlxGAkOd4Q"
      },
      "outputs": [],
      "source": [
        "model.output_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVtJ1Q7nNcSZ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_PpxPtNOSa1"
      },
      "outputs": [],
      "source": [
        "cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: cw[i] for i in range(len(cw))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0an8O-SVX_C"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_model(\n",
        "    model,\n",
        "    to_file='bilstm_attention_model.png',\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB',\n",
        "    dpi=120\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=6,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        factor=0.4,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,{\"pred\": y_train, \"att_weights\": y_train},\n",
        "    validation_data=(X_val, {\"pred\": y_val, \"att_weights\": y_val}),\n",
        "    epochs=40,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "history_dict = history.history\n",
        "train_acc = history_dict['pred_accuracy']\n",
        "val_acc   = history_dict['val_pred_accuracy']\n",
        "train_loss = history_dict['loss']\n",
        "val_loss   = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bGkRvAObiAYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "train_acc = history_dict['pred_accuracy']\n",
        "val_acc   = history_dict['val_pred_accuracy']\n",
        "train_loss = history_dict['loss']\n",
        "val_loss   = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "asxn_vxo6ZFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zC-q6OlwFI5"
      },
      "outputs": [],
      "source": [
        "test_results = model.evaluate(\n",
        "    x_test,\n",
        "    {\"pred\": y_test, \"att_weights\": y_test},\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "test_acc = test_results[-1]\n",
        "\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "model.save(\"bilstm_attention.keras\")\n",
        "model.save(\"bilstm_attention.h5\")\n",
        "model.save_weights(\"bilstm_attention.weights.h5\")\n",
        "print(\"Saved to content \")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"bilstm_attention.h5\")\n",
        "files.download(\"bilstm_attention.keras\")\n",
        "files.download(\"bilstm_attention.weights.h5\")\n",
        "print(\"Downloaded to Local\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEJnFCkcwK8U"
      },
      "outputs": [],
      "source": [
        "y_pred, att_weights = model.predict(x_test[16:17])\n",
        "att_weights = att_weights.squeeze()\n",
        "plt.plot(att_weights)\n",
        "plt.title(\"Attention over 48 hours\")\n",
        "plt.xlabel(\"Hour index (0–47)\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LvAvAuHJLyn"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(x_test)[0]\n",
        "cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test, y_pred_classes, digits=4))\n"
      ],
      "metadata": {
        "id": "XWWfykDJtkdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "pd.DataFrame({'Class': unique, 'Count': counts, 'Percent': counts / counts.sum() * 100})\n"
      ],
      "metadata": {
        "id": "XEwY2I6uu1wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics"
      ],
      "metadata": {
        "id": "XnAbc1nmIagE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, balanced_accuracy_score, matthews_corrcoef,\n",
        "    log_loss, roc_auc_score, precision_recall_curve, auc,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "MoKOENU4Ivvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = model.predict(x_test)\n",
        "if isinstance(y_proba, (list, tuple)):\n",
        "    y_proba = y_proba[0]\n",
        "y_pred = np.argmax(y_proba, axis=1)\n",
        "y_true = y_test"
      ],
      "metadata": {
        "id": "EQwy_oq9IaHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_true, y_pred)\n",
        "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "mcc = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Macro-F1:\", macro_f1)\n",
        "print(\"Micro-F1:\", micro_f1)\n",
        "print(\"Balanced Accuracy:\", balanced_acc)\n",
        "print(\"MCC:\", mcc)\n"
      ],
      "metadata": {
        "id": "sNgFIkJWI3ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ll = log_loss(y_true, y_proba)\n",
        "brier = np.mean(np.sum((y_proba - np.eye(3)[y_true])**2, axis=1))\n",
        "\n",
        "print(\"Log Loss:\", ll)\n",
        "print(\"Brier Score:\", brier)\n"
      ],
      "metadata": {
        "id": "6rwHjTDPI8XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc_macro = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n",
        "auc_micro = roc_auc_score(y_true, y_proba, multi_class='ovr', average='micro')\n",
        "\n",
        "print(\"Macro ROC-AUC:\", auc_macro)\n",
        "print(\"Micro ROC-AUC:\", auc_micro)\n"
      ],
      "metadata": {
        "id": "eQbQRc4yI-Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pr_auc = {}\n",
        "for c in range(3):\n",
        "    y_bin = (y_true == c).astype(int)\n",
        "    precision, recall, _ = precision_recall_curve(y_bin, y_proba[:, c])\n",
        "    pr_auc[c] = auc(recall, precision)\n",
        "\n",
        "print(\"PR-AUC:\", pr_auc)\n"
      ],
      "metadata": {
        "id": "UU8vC7dSI_4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Normalized Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8-LPqTLeJBna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "def expected_calibration_error(y_true, y_proba, n_bins=15):\n",
        "    probs = y_proba.max(axis=1)\n",
        "    preds = np.argmax(y_proba, axis=1)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    ece = 0.0\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        idx = (probs > bins[i]) & (probs <= bins[i+1])\n",
        "        if np.any(idx):\n",
        "            acc = np.mean(preds[idx] == y_true[idx])\n",
        "            conf = np.mean(probs[idx])\n",
        "            ece += (np.sum(idx) / len(y_true)) * np.abs(acc - conf)\n",
        "    return ece\n",
        "\n",
        "pred_conf = y_proba.max(axis=1)\n",
        "pred_label = np.argmax(y_proba, axis=1)\n",
        "correct = (pred_label == y_true).astype(int)\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(correct, pred_conf, n_bins=15)\n",
        "\n",
        "plt.plot(prob_pred, prob_true, marker='o')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Observed Frequency\")\n",
        "plt.title(\"Calibration Curve\")\n",
        "plt.show()\n",
        "\n",
        "print(\"ECE:\", expected_calibration_error(y_true, y_proba))\n"
      ],
      "metadata": {
        "id": "wEtc68jhJD3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_ci(y_true, y_pred, metric, n=2000):\n",
        "    stats = []\n",
        "    N = len(y_true)\n",
        "    for _ in range(n):\n",
        "        idx = np.random.randint(0, N, N)\n",
        "        stats.append(metric(y_true[idx], y_pred[idx]))\n",
        "    return np.percentile(stats, [2.5, 97.5])\n",
        "\n",
        "acc_ci = bootstrap_ci(np.array(y_true), np.array(y_pred), accuracy_score)\n",
        "f1_ci = bootstrap_ci(np.array(y_true), np.array(y_pred),\n",
        "                     lambda a, b: f1_score(a, b, average='macro'))\n",
        "\n",
        "print(\"Accuracy 95% CI:\", acc_ci)\n",
        "print(\"Macro-F1 95% CI:\", f1_ci)\n"
      ],
      "metadata": {
        "id": "LpHvnAmYJIGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "tb = np.zeros((2,2))\n",
        "\n",
        "for yt, yp_b, yp_o in zip(y_true, y_pred_b, y_pred):\n",
        "    tb[int(yp_b == yt)][int(yp_o == yt)] += 1\n",
        "\n",
        "result = mcnemar(tb, exact=False, correction=True)\n",
        "print(\"McNemar p-value:\", result.pvalue)\n",
        "print(\"Contingency table:\\n\", tb)\n"
      ],
      "metadata": {
        "id": "92kTzC7hJNQm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}